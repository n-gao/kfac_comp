{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from plotting import plot_3d, plot_2d\n",
    "from tf2kfac import tf2KFAC\n",
    "import tensorflow_probability as tfp\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.convert_to_tensor(np.load('data/input.npy').astype(np.float32))\n",
    "weights = np.load('data/weights.npy').astype(np.float32)\n",
    "labels = tf.convert_to_tensor(np.load('data/labels.npy').astype(np.int32))\n",
    "labels = tf.reshape(labels, (-1,))\n",
    "\n",
    "input_data = tf.convert_to_tensor(input_data)\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets compare the pure loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.convert_to_tensor(np.load('data/input.npy'), dtype=tf.float32)\n",
    "weights = tf.convert_to_tensor(np.load('data/weights.npy'), dtype=tf.float32)\n",
    "labels = tf.convert_to_tensor(np.load('data/labels.npy'), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factored Tikhonov damping\n",
      "(16, 2)\n",
      "Loss tf.Tensor(1.1128990160589591, shape=(), dtype=float64)\n",
      "Gradients tf.Tensor(\n",
      "[[-0.39162274  0.39162274]\n",
      " [ 0.08551887 -0.08551887]], shape=(2, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "input_data = tf.convert_to_tensor(np.load('data/input.npy'))\n",
    "weights = tf.convert_to_tensor(np.load('data/weights.npy'))\n",
    "labels = tf.convert_to_tensor(np.load('data/labels.npy'))\n",
    "labels = tf.reshape(labels, (-1,))\n",
    "\n",
    "kfac = tf2KFAC(tinv=10,\n",
    "               lr0=0.005,\n",
    "               cov_weight=1.,\n",
    "               cov_moving_weight=1.,\n",
    "               damping=0.001,\n",
    "               conv_approx='mg',\n",
    "               damping_method='factored_tikhonov',\n",
    "               ft_method='original')\n",
    "\n",
    "\n",
    "# need a softmax layer to define the output distribution p(y | x) to compute the fisher\n",
    "print(input_data.shape)\n",
    "with tf.GradientTape(True) as g:\n",
    "    g.watch(weights)\n",
    "    logits = input_data @ weights\n",
    "    output_dist = tf.nn.softmax(logits)\n",
    "    neg_log_prob = -tf.math.log(output_dist)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "sensitivities = g.gradient(neg_log_prob, logits)\n",
    "grads = g.gradient(loss, weights)\n",
    "print('Loss', loss)\n",
    "print('Gradients', grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dist = tfp.distributions.Normal(0.0, tfsqrt(0.5))\n",
    "\n",
    "history = []\n",
    "for i in range(1):\n",
    "    activations = input_data\n",
    "    with tf.GradientTape(persistent=True) as g:\n",
    "        g.watch(weights)\n",
    "        prediction = input_data@weights\n",
    "        model_dist = tf.reduce_mean((prediction - target)**2)\n",
    "        log_prob = - dist.log_prob(prediction)\n",
    "        loss = \n",
    "    sensitivities = g.gradient(log_prob, prediction)\n",
    "    \n",
    "    grads = g.gradient(loss, weights)\n",
    "    \n",
    "    history.append(np.concatenate([weights.numpy().reshape(-1), loss.numpy()[None]]))\n",
    "    ng = kfac.compute_updates([activations], [sensitivities], [grads], i)\n",
    "    weights = weights + ng[0]\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        print(i, 'Loss', loss)\n",
    "history = np.array(history, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kfac.m_aa[0])\n",
    "print(kfac.m_ss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plot_3d(history, input_data, target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.title('TF2 KFAC')\n",
    "plot_2d(history, input_data, target, (-0.5, 1, -3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
