{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets compare the pure loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.convert_to_tensor(np.load('data/input.npy'))\n",
    "weights = tf.convert_to_tensor(np.load('data/weights.npy'))\n",
    "target = tf.convert_to_tensor(np.load('data/target.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tf.Tensor(9.8538375763679, shape=(), dtype=float64)\n",
      "Gradients tf.Tensor(\n",
      "[[ 0.63178222]\n",
      " [ 1.60616247]\n",
      " [ 2.99499008]\n",
      " [-2.06971753]\n",
      " [-0.81690381]\n",
      " [ 0.07546019]\n",
      " [-2.35599685]\n",
      " [ 6.14043213]], shape=(8, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# need a softmax layer to define the output distribution p(y | x) to compute the fisher\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(weights)\n",
    "    loss = tf.reduce_mean(((input_data@weights) - target)**2)  \n",
    "print('Loss', loss)\n",
    "print('Gradients', g.gradient(loss, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf2KFAC():\n",
    "    def __init__(self,\n",
    "                 lr0=0.0001,\n",
    "                 cov_weight=1.,\n",
    "                 cov_moving_weight=0.95, \n",
    "                 damping=0.001, \n",
    "                 conv_approx='mg', \n",
    "                 damping_method='factored_tikhonov',\n",
    "                 ft_method='original',\n",
    "                norm_constraint=0.95):\n",
    "        \n",
    "        self.cov_weight = cov_weight  \n",
    "        # cov weight 1. appears to be the default setting in tf1 code\n",
    "        # set to 0.05 to match david pfaus paper, though I believe they didnt realise\n",
    "        # it changes which changes the relative value of cov moving weight\n",
    "        \n",
    "        self.cov_moving_weight = cov_moving_weight # the is set to 0.95 as an input \n",
    "        self.damping = damping\n",
    "        self.lr0 = lr0\n",
    "        self.lr = lr0\n",
    "        self.decay = 1e-4\n",
    "        self.norm_constraint = norm_constraint\n",
    "        \n",
    "        # dictionaries holding the moving averages\n",
    "        self.m_aa = {}\n",
    "        self.m_ss = {}\n",
    "        self.n_spatial_locations = {}  # n_conv, n_spatial_locations etc\n",
    "        # this prep really depends on how we arrange the layers so I have hardcoded sizes here \n",
    "        n_dim_in_0 = 10\n",
    "        n_dim_out_0 = 5\n",
    "        self.layers = [0] \n",
    "        self.n_spatial_locations = [1.]\n",
    "        for layer in self.layers:\n",
    "            self.m_aa[layer] = tf.ones((n_dim_in_0, n_dim_in_0))\n",
    "            self.m_ss[layer] = tf.ones((n_dim_out_0, n_dim_out_0))\n",
    "        \n",
    "        # for later when we have conv layers\n",
    "        # these change depending on the approximation \n",
    "        # and may change depending on if is sensitivities or activations\n",
    "        if conv_approx == 'mg':  # martens grosse 2015\n",
    "            self.compute_norm_m_xx = lambda n, cv: float(n * cv)\n",
    "            self.compute_norm_g = lambda cv: cv\n",
    "            self.compute_grad_norm = lambda cv: cv\n",
    "        \n",
    "        \n",
    "        # contain different damping methods and computations of the nat grads\n",
    "        if damping_method == 'factored_tikhonov':\n",
    "            self.ops = FactoredTikhonov(ft_method)\n",
    "        elif damping_method == 'tikhonov':\n",
    "            self.ops = Tikhonov()\n",
    "            \n",
    "    def compute_lr(self, iteration):\n",
    "        return self.lr0 / (1 + self.decay * iteration)     \n",
    "    \n",
    "    def compute_updates(self, activations, sensitivities, grads, iteration):\n",
    "        \n",
    "        # update the cov_moving weight dependent on iteration\n",
    "        cov_moving_weight = tf.minimum(1. - 1. / (1. + iteration), self.cov_moving_weight)\n",
    "        cov_moving_normalize = self.cov_weight + cov_moving_weight\n",
    "        \n",
    "        # update the learning rate\n",
    "        self.lr = self.compute_lr(iteration)\n",
    "\n",
    "        nat_grads = []\n",
    "        for l, a, s, g, cv in zip(self.layers, activations, sensitivities, grads, self.n_spatial_locations):\n",
    "            n_samples = a.shape[0]\n",
    "            \n",
    "            # different normaizations for different approximations\n",
    "            norm_a = self.compute_norm_m_xx(n_samples, cv)\n",
    "            norm_s = self.compute_norm_m_xx(n_samples, cv)\n",
    "            grad_norm = self.compute_grad_norm(cv)\n",
    "            \n",
    "            # absorb the spatial dim when we have conv layers\n",
    "            a = self.absorb_cv(a, l)\n",
    "            s = self.absorb_cv(s, l)\n",
    "            \n",
    "            # compute the expectation of the kronecker factors\n",
    "            aa = self.outer_product(a, l) / norm_a \n",
    "            ss = self.outer_product(s, l) / norm_s\n",
    "            \n",
    "            # update the moving averages\n",
    "            self.update_m_aa_and_m_ss(aa, ss, cov_moving_weight, cov_moving_normalize, l, iteration)\n",
    "            \n",
    "            # computes the nat grads depending on the damping method\n",
    "            ng = self.ops.compute_nat_grads(self.m_aa[l], \n",
    "                                            self.m_ss[l], \n",
    "                                            g, \n",
    "                                            grad_norm, \n",
    "                                            self.damping, \n",
    "                                            l, \n",
    "                                            iteration)\n",
    "            \n",
    "            \n",
    "            nat_grads.append(ng)\n",
    "        \n",
    "        # compute the norm constraint \n",
    "        eta = self.compute_norm_constraint(nat_grads, grads)\n",
    "        \n",
    "        # apply the learning rate to the updates - in the kfac code this is applied inside an external optimiser \n",
    "        # so if we want to compare the nat grads we may have to remove self.lr here and compare the raw grads\n",
    "        updates = [-1. * eta * self.lr * ng for ng in nat_grads]\n",
    "        \n",
    "        return updates\n",
    "    \n",
    "    # m_xx = (cov_moving_weight * m_xx + cov_weight * xx)  / normalization\n",
    "    def update_m_aa_and_m_ss(self, aa, ss, cov_moving_weight, cov_normalize, layer, iteration):\n",
    "\n",
    "        # tensorflow and or ray has a weird thing about inplace operations?? make sure these are updating \n",
    "        self.m_aa[layer] *= cov_moving_weight / cov_normalize  # multiply\n",
    "        self.m_aa[layer] += (self.cov_weight * aa) / cov_normalize  # add\n",
    "\n",
    "        self.m_ss[layer] *= cov_moving_weight / cov_normalize\n",
    "        self.m_ss[layer] += (self.cov_weight * ss) / cov_normalize\n",
    "        return\n",
    "        \n",
    "    def compute_norm_constraint(self, nat_grads, grads):\n",
    "        sq_fisher_norm = 0\n",
    "        for ng, g in zip(nat_grads, grads):\n",
    "            sq_fisher_norm += tf.reduce_sum(ng * g)\n",
    "        eta = tf.minimum(1., tf.sqrt(self.norm_constraint / (self.lr**2 * sq_fisher_norm)))\n",
    "        return eta\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def outer_product(x, l):\n",
    "        return tf.matmul(x, x, transpose_a=True)  # can change this for when we have conv layers\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def absorb_cv(x, l):  # can change to absorb the conv dimension via reshapes\n",
    "        return x\n",
    "    \n",
    "def compute_eig_decomp(maa, mss):\n",
    "    # enforce symmetry \n",
    "    maa = (tf.linalg.matrix_transpose(maa) + maa) / 2\n",
    "    mss = (tf.linalg.matrix_transpose(mss) + mss) / 2\n",
    "    \n",
    "    # get the eigenvalues and eigenvectors of a symmetric positive matrix\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        vals_a, vecs_a = tf.linalg.eigh(maa)\n",
    "        vals_s, vecs_s = tf.linalg.eigh(mss)\n",
    "\n",
    "    # zero negative eigenvalues. eigh outputs VALUES then VECTORS\n",
    "    # print('zero')\n",
    "    # print(vals_a.shape, vecs_a.shape)\n",
    "    vals_a = tf.maximum(vals_a, tf.zeros_like(vals_a))\n",
    "    vals_s = tf.maximum(vals_s, tf.zeros_like(vals_s))\n",
    "\n",
    "    return vals_a, vecs_a, vals_s, vecs_s\n",
    "    \n",
    "class Tikhonov():\n",
    "    def __init__(self):\n",
    "        print('Tikhonov damping')\n",
    "\n",
    "    def compute_nat_grads(self,  maa, mss, g, grad_norm, damping, layer, iteration):\n",
    "        \n",
    "        vals_a, vecs_a, vals_s, vecs_s = compute_eig_decomp(maa, mss)\n",
    "        \n",
    "        v1 = tf.linalg.matmul(vecs_a, g / grad_norm, transpose_a=True) @ vecs_s\n",
    "        divisor = tf.expand_dims(vals_s, -2) * tf.expand_dims(vals_a, -1)\n",
    "        v2 = v1 / (divisor + damping / grad_norm)  # comes from pulling the lambda out cv*F + \\lambda = cv*(F + \\lambda / cv) \n",
    "        ng = vecs_a @ tf.linalg.matmul(v2, vecs_s, transpose_b=True)\n",
    "\n",
    "        return ng\n",
    "\n",
    "class FactoredTikhonov():\n",
    "    def __init__(self, ft_method):\n",
    "        print('Factored Tikhonov damping')\n",
    "        self.ft_method = 'original'  # this is what is in the code, in the theory pi can be set to 1\n",
    "\n",
    "\n",
    "    def compute_nat_grads(self,  maa, mss, g, grad_norm, damping, layer, iteration):\n",
    "\n",
    "        maa, mss = self.damp(maa, mss, grad_norm, damping, layer, iteration)\n",
    "\n",
    "        vals_a, vecs_a, vals_s, vecs_s = compute_eig_decomp(maa, mss)\n",
    "\n",
    "        v1 = tf.linalg.matmul(vecs_a, g / grad_norm, transpose_a=True) @ vecs_s\n",
    "        divisor = tf.expand_dims(vals_s, -2) * tf.expand_dims(vals_a, -1)\n",
    "        v2 = v1 / divisor\n",
    "        ng = vecs_a @ tf.linalg.matmul(v2, vecs_s, transpose_b=True)\n",
    "\n",
    "        return ng\n",
    "\n",
    "    def damp(self, m_aa, m_ss, grad_norm, damping, name, iteration):  # factored tikhonov damping\n",
    "        dim_a = m_aa.shape[-1]\n",
    "        dim_s = m_ss.shape[-1]\n",
    "        batch_shape = list((1 for _ in m_aa.shape[:-2]))  # needs to be cast as list or disappears in tf.eye\n",
    "\n",
    "        if self.ft_method == 'ones_pi':\n",
    "            pi = tf.expand_dims(tf.expand_dims(tf.ones(batch_shape), -1), -1)\n",
    "        else:\n",
    "            tr_a = self.get_tr_norm(m_aa)\n",
    "            tr_s = self.get_tr_norm(m_ss)\n",
    "            pi = tf.expand_dims(tf.expand_dims((tr_a * dim_s) / (tr_s * dim_a), -1), -1)\n",
    "\n",
    "\n",
    "        eye_a = tf.eye(dim_a, batch_shape=batch_shape)\n",
    "        eye_s = tf.eye(dim_s, batch_shape=batch_shape)\n",
    " \n",
    "        m_aa_damping = tf.sqrt(pi * damping / grad_norm) # comes from pulling the lambda out cv*F + \\lambda = cv*(F + \\lambda / cv) \n",
    "        m_ss_damping = tf.sqrt(damping / (pi * grad_norm))\n",
    "\n",
    "        m_aa += eye_a * m_aa_damping\n",
    "        m_ss += eye_s * m_ss_damping\n",
    "\n",
    "        return m_aa, m_ss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tr_norm(m_xx):\n",
    "        trace = tf.linalg.trace(m_xx)\n",
    "        # need to double check what the default min value is from the original code\n",
    "        return tf.maximum(1e-10 * tf.ones_like(trace), trace)\n",
    "        \n",
    "n_samples = 12\n",
    "activations = [tf.random.normal((n_samples, 10))]\n",
    "sensitivities = [tf.random.normal((n_samples, 5))]\n",
    "grads = [tf.random.normal((10, 5))]\n",
    "iteration = 1\n",
    "\n",
    "# tf.matmul(activations, activations, transpose_a=True)\n",
    "kfac = tf2KFAC(lr0=0.0001,\n",
    "                 cov_weight=1.,\n",
    "                 cov_moving_weight=0.95, \n",
    "                 damping=0.001, \n",
    "                 conv_approx='mg', \n",
    "                 damping_method='factored_tikhonov',\n",
    "                ft_method='original')\n",
    "\n",
    "ng = kfac.compute_updates(activations, sensitivities, grads, iteration)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factored Tikhonov damping\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
