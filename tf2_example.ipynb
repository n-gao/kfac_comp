{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OrdereredDict' from 'collections' (/home/maxiwelian/anaconda3/envs/aqua/lib/python3.7/collections/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-258ece4b0fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrdereredDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OrdereredDict' from 'collections' (/home/maxiwelian/anaconda3/envs/aqua/lib/python3.7/collections/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrdereredDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets compare the pure loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.convert_to_tensor(np.load('data/input.npy'))\n",
    "weights = tf.convert_to_tensor(np.load('data/weights.npy'))\n",
    "target = tf.convert_to_tensor(np.load('data/target.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tf.Tensor(9.853837576367901, shape=(), dtype=float64)\n",
      "Gradients tf.Tensor(\n",
      "[[ 0.63178222]\n",
      " [ 1.60616247]\n",
      " [ 2.99499008]\n",
      " [-2.06971753]\n",
      " [-0.81690381]\n",
      " [ 0.07546019]\n",
      " [-2.35599685]\n",
      " [ 6.14043213]], shape=(8, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# need a softmax layer to define the output distribution p(y | x) to compute the fisher\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(weights)\n",
    "    loss = tf.reduce_mean(((input_data@weights) - target)**2)  \n",
    "print('Loss', loss)\n",
    "print('Gradients', g.gradient(loss, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'static_method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2d662e54aefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mtf2KFAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(lr0=0.0001,\n\u001b[1;32m      3\u001b[0m                  \u001b[0mcov_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mcov_moving_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mdamping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2d662e54aefc>\u001b[0m in \u001b[0;36mtf2KFAC\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstatic_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mouter_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# can change this for when we have conv layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'static_method' is not defined"
     ]
    }
   ],
   "source": [
    "class tf2KFAC():\n",
    "    def __init__(self,\n",
    "                 lr0=0.0001,\n",
    "                 cov_weight=1.,\n",
    "                 cov_moving_weight=0.95, \n",
    "                 damping=0.001, \n",
    "                 conv_approx='mg', \n",
    "                 damping_method='factored_tikhonov'):\n",
    "        \n",
    "        self.cov_weight = cov_weight  \n",
    "        # cov weight 1. appears to be the default setting in tf1 code\n",
    "        # set to 0.05 to match david pfaus paper, though I believe they didnt realise\n",
    "        # it changes which changes the relative value of cov moving weight\n",
    "        \n",
    "        self.cov_moving_weight = cov_moving_weight # the is set to 0.95 as an input \n",
    "        self.damping = damping\n",
    "        self.lr0 = lr0\n",
    "        self.lr = lr0\n",
    "        \n",
    "        # dictionaries holding the moving averages\n",
    "        self.m_aa = {}\n",
    "        self.m_ss = {}\n",
    "        self.n_spatial_locations = {}  # n_conv, n_spatial_locations etc\n",
    "        # this prep really depends on how we arrange the layers so I have hardcoded sizes here \n",
    "        n_dim_in_0 = 10\n",
    "        n_dim_out_0 = 5\n",
    "        self.layers = [0] \n",
    "        for layer in self.layers:\n",
    "            self.m_aa[layer] = tf.ones((n_dim_in_0, n_dim_in_0))\n",
    "            self.m_aa[layer] = tf.ones((n_dim_out_0, n_dim_out_0))\n",
    "            self.n_spatial_locations[layer] = 1.  # or n_dim_conv\n",
    "        \n",
    "        # for later when we have conv layers\n",
    "        # these change depending on the approximation \n",
    "        # and may change depending on if is sensitivities or activations\n",
    "        if conv_approx == 'mg':  # martens grosse 2015\n",
    "            self.compute_norm_m_xx = lambda n, cv: float(n * cv)\n",
    "            self.compute_norm_g = lambda cv: cv\n",
    "            self.compute_grad_norm = lambda cv: cv\n",
    "        \n",
    "        \n",
    "        # contain different damping methods and computations of the nat grads\n",
    "        if damping_method == 'factored_tikhonov':\n",
    "                self.ops = FactoredTikhonov()\n",
    "        elif damping_method == 'tikhonov':\n",
    "            self.ops = Tikhonov()\n",
    "            \n",
    "    def compute_lr(self, iteration):\n",
    "        return self.lr0 / (1 + self.decay * iteration)     \n",
    "    \n",
    "    def compute_updates(self, activations, sensitivities, grads, iteration):\n",
    "        \n",
    "        # update the cov_moving weight dependent on iteration\n",
    "        cov_moving_weight = tf.minimum(1. - 1. / (1. + iteration), self.cov_moving_weight)\n",
    "        cov_moving_normalize = cov_weight + cov_moving_weight\n",
    "        \n",
    "        # update the learning rate\n",
    "        self.lr = self.compute_lr(iteration)\n",
    "        \n",
    "        nat_grads = []\n",
    "        for l, a, s, g, cv in zip(self.layers, activations, sensitivities, grads, self.n_spatial_locations):\n",
    "            n_samples = a.shape[0]\n",
    "            \n",
    "            norm_a = self.compute_norm_m_xx(n_samples, cv)\n",
    "            norm_s = self.compute_norm_m_xx(n_samples, cv)\n",
    "            grad_norm = self.self.compute_grad_norm(cv)\n",
    "            \n",
    "            a = self.absorb_cv(a, l)\n",
    "            s = self.absorb_cv(s, l)\n",
    "            \n",
    "            \n",
    "            # update the moving averages\n",
    "            self.m_aa[l] = self.outer_product(a, l) / norm_a \n",
    "            self.m_ss[l] = self.outer_product(s, l) / norm_s\n",
    "            \n",
    "            # computes the nat grads depending on the damping method\n",
    "            ng = self.ops.compute_nat_grads(self.m_aa[l], \n",
    "                                            self.m_ss[l], \n",
    "                                            g, \n",
    "                                            grad_norm, \n",
    "                                            self.damping, \n",
    "                                            layer, \n",
    "                                            iteration)\n",
    "            \n",
    "            \n",
    "            nat_grads.append(ng)\n",
    "        \n",
    "        # compute the norm constraint \n",
    "        eta = self.compute_norm_constraint(nat_grads, grads)\n",
    "        \n",
    "        # apply the learning rate to the updates - in the kfac code this is applied inside an external optimiser \n",
    "        # so if we want to compare the nat grads we may have to remove self.lr here and compare the raw grads\n",
    "        updates = [-1. * eta * self.lr * ng for ng in nat_grads]\n",
    "        \n",
    "        return updates\n",
    "        \n",
    "        \n",
    "    def compute_norm_constraint(self, nat_grads, grads):\n",
    "        sq_fisher_norm = 0\n",
    "        for ng, g in zip(nat_grads, grads):\n",
    "            sq_fisher_norm += tf.reduce_sum(ng * g)\n",
    "        eta = tf.minimum(1., tf.sqrt(self.norm_constraint / (self.lr**2 * sq_fisher_norm)))\n",
    "        return eta\n",
    "        \n",
    "        \n",
    "    @static_method\n",
    "    def outer_product(x, l):\n",
    "        return tf.matmul(x, x, transpose_a=True)  # can change this for when we have conv layers\n",
    "    \n",
    "    \n",
    "    @static_method\n",
    "    def absorb_cv(x, l):  # can change to absorb the conv dimension via reshapes\n",
    "        return x\n",
    "    \n",
    "def compute_eig_decomp(self, maa, mss):\n",
    "    # enforce symmetry \n",
    "    m_aa = (tf.linalg.matrix_transpose(m_aa) + m_aa) / 2\n",
    "    m_ss = (tf.linalg.matrix_transpose(m_ss) + m_ss) / 2\n",
    "\n",
    "    # get the eigenvalues and eigenvectors of a symmetric positive matrix\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        vals_a, vecs_a = tf.linalg.eigh(maa)\n",
    "        vals_s, vecs_s = tf.linalg.eigh(mss)\n",
    "\n",
    "    # zero negative eigenvalues. eigh outputs VALUES then VECTORS\n",
    "    # print('zero')\n",
    "    # print(vals_a.shape, vecs_a.shape)\n",
    "    vals_a = tf.maximum(vals_a, tf.zeros_like(vals_a))\n",
    "    vals_s = tf.maximum(vals_s, tf.zeros_like(vals_s))\n",
    "\n",
    "    return vals_a, vecs_a, vals_s, vecs_s\n",
    "    \n",
    "class Tikhonov():\n",
    "    def __init__(self):\n",
    "        print('Tikhonov damping')\n",
    "\n",
    "    def compute_nat_grads(self,  maa, mss, g, grad_norm, damping, layer, iteration):\n",
    "        \n",
    "        vals_a, vecs_a, vals_s, vecs_s = compute_eig_decomp(maa, mss)\n",
    "        \n",
    "        v1 = tf.linalg.matmul(vecs_a, g / grad_norm, transpose_a=True) @ vecs_s\n",
    "        divisor = tf.expand_dims(vals_s, -2) * tf.expand_dims(vals_a, -1)\n",
    "        v2 = v1 / (divisor + damping / grad_norm)  # comes from pulling the lambda out cv*F + \\lambda = cv*(F + \\lambda / cv) \n",
    "        ng = vecs_a @ tf.linalg.matmul(v2, vecs_s, transpose_b=True)\n",
    "\n",
    "        return ng\n",
    "\n",
    "class FactoredTikhonov():\n",
    "    def __init__(self, ft_method):\n",
    "        print('Factored Tikhonov damping')\n",
    "        self.ft_method = 'original'  # this is what is in the code, in the theory pi can be set to 1\n",
    "\n",
    "\n",
    "    def compute_nat_grads(self,  maa, mss, g, grad_norm, damping, layer, iteration):\n",
    "\n",
    "        maa, mss = self.damp(maa, mss, grad_norm, damping, layer, iteration)\n",
    "\n",
    "        vals_a, vecs_a, vals_s, vecs_s = compute_eig_decomp(maa, mss)\n",
    "\n",
    "        v1 = tf.linalg.matmul(vecs_a, g / grad_norm, transpose_a=True) @ vecs_s\n",
    "        divisor = tf.expand_dims(vals_s, -2) * tf.expand_dims(vals_a, -1)\n",
    "        v2 = v1 / divisor\n",
    "        ng = vecs_a @ tf.linalg.matmul(v2, vecs_s, transpose_b=True)\n",
    "\n",
    "        return ng\n",
    "\n",
    "    def damp(self, m_aa, m_ss, grad_norm, damping, name, iteration):  # factored tikhonov damping\n",
    "        dim_a = m_aa.shape[-1]\n",
    "        dim_s = m_ss.shape[-1]\n",
    "        batch_shape = list((1 for _ in m_aa.shape[:-2]))  # needs to be cast as list or disappears in tf.eye\n",
    "\n",
    "        if self.ft_method == 'ones_pi':\n",
    "            pi = tf.expand_dims(tf.expand_dims(tf.ones(batch_shape), -1), -1)\n",
    "        else:\n",
    "            tr_a = self.get_tr_norm(m_aa)\n",
    "            tr_s = self.get_tr_norm(m_ss)\n",
    "            pi = tf.expand_dims(tf.expand_dims((tr_a * dim_s) / (tr_s * dim_a), -1), -1)\n",
    "\n",
    "\n",
    "        eye_a = tf.eye(dim_a, batch_shape=batch_shape)\n",
    "        eye_s = tf.eye(dim_s, batch_shape=batch_shape)\n",
    " \n",
    "        m_aa_damping = tf.sqrt(pi * damping / grad_norm) # comes from pulling the lambda out cv*F + \\lambda = cv*(F + \\lambda / cv) \n",
    "        m_ss_damping = tf.sqrt(damping / (pi * grad_norm))\n",
    "\n",
    "        m_aa += eye_a * m_aa_damping\n",
    "        m_ss += eye_s * m_ss_damping\n",
    "\n",
    "        return m_aa, m_ss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tr_norm(m_xx):\n",
    "        trace = tf.linalg.trace(m_xx)\n",
    "        # need to double check what the default min value is from the original code\n",
    "        return tf.maximum(1e-10 * tf.ones_like(trace), trace)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
